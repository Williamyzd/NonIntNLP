{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import orjson\n",
    "import transformers\n",
    "import torch\n",
    "import wandb\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "fp16 = True\n",
    "if fp16:\n",
    "    from apex import amp\n",
    "\n",
    "model_name = \"xlnet-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(input_file, _max_seq_len):\n",
    "    data = orjson.loads(open(input_file, \"rb\").read())\n",
    "    # Root data is a list of lists of features. The first-order list organizes the sequences into sets of like-length \n",
    "    # that can be batched together.\n",
    "    datasets = []\n",
    "    count = 0\n",
    "    for features in data:\n",
    "        # Each feature is a dictionary of a 'text' sequence and a 'title' sequence. The goal of this model is to\n",
    "        # predict the 'title' given the 'text'. Process them out together, the model trainer will do the rest of the\n",
    "        # work.\n",
    "        input_ids = torch.tensor([f['text']['input_ids'] for f in features], dtype=torch.long)\n",
    "        attention_mask = torch.tensor([f['text']['attention_mask'] for f in features], dtype=torch.float)\n",
    "        token_type_ids = torch.tensor([f['text']['token_type_ids'] for f in features], dtype=torch.long)\n",
    "        title_input_ids = torch.tensor([f['title']['input_ids'] for f in features], dtype=torch.long)\n",
    "        datasets.append(TensorDataset(input_ids, attention_mask, token_type_ids, title_input_ids))\n",
    "        # This trainer \"chunks\" the dataset lower and trains per-chunk.\n",
    "        _num_chunks = input_ids.shape[-1] / _max_seq_len\n",
    "        count += len(features) * _num_chunks\n",
    "    return datasets, count\n",
    "\n",
    "# Process dataset\n",
    "input_folder = \"C:\\\\Users\\\\jbetk\\\\Documents\\\\data\\\\ml\\\\title_prediction\\\\outputs\\\\\"\n",
    "train_datasets, total_train_data_sz = load_dataset(input_folder + \"processed.json\", 128)\n",
    "val_datasets, total_val_data_sz = load_dataset(input_folder + \"validation.json\", 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/neonbjb/nonint-transformers-torch\" target=\"_blank\">https://app.wandb.ai/neonbjb/nonint-transformers-torch</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/neonbjb/nonint-transformers-torch/runs/bsptsy3o\" target=\"_blank\">https://app.wandb.ai/neonbjb/nonint-transformers-torch/runs/bsptsy3o</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.29 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 4\n",
    "CHUNK_SEQ_LEN = 128\n",
    "TITLE_PRED_MAX_LEN = 64\n",
    "\n",
    "# Load model\n",
    "tokenizer = transformers.XLNetTokenizer.from_pretrained(model_name)\n",
    "config = transformers.XLNetConfig.from_pretrained(model_name)\n",
    "config.mem_len = 1024\n",
    "model = transformers.XLNetLMHeadModel.from_pretrained(model_name, config=config)\n",
    "device = torch.device(\"cuda\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0,\n",
    "    },\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
    "                                                         num_warmup_steps=0, num_training_steps=EPOCHS * int(total_train_data_sz))\n",
    "\n",
    "# Shift model to cuda & enable fp16 if applicable.\n",
    "model.to(device)\n",
    "if fp16:\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "    \n",
    "# Initialize w&b logger\n",
    "do_wandb = True\n",
    "if do_wandb:\n",
    "    wandb.init(project=\"nonint-transformers-torch\",\\\n",
    "               name=\"xlnet_title_prediction\",\\\n",
    "               config={\"dataset\": \"title_pred\"})\n",
    "    # There's something bugged about this, but it doesnt really seem to do much anyways. Apparently it enables some \n",
    "    # sort of gradient exploration map.\n",
    "    #wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-fb198941bb32>\"\u001b[1;36m, line \u001b[1;32m19\u001b[0m\n\u001b[1;33m    for b in range()\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ],
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-fb198941bb32>, line 19)",
     "output_type": "error"
    }
   ],
   "source": [
    "preprocess_times = []\n",
    "xfer_times = []\n",
    "forward_times = []\n",
    "backward_times = []\n",
    "opt_times = []\n",
    "sched_times = []\n",
    "\n",
    "def clear_timers():\n",
    "    xfer_times.clear()\n",
    "    forward_times.clear()\n",
    "    backward_times.clear()\n",
    "    opt_times.clear()\n",
    "    sched_times.clear()\n",
    "\n",
    "\n",
    "def random_mask(_inputs_to_mask, _tokenizer, _proportion_to_mask=.2):\n",
    "    with torch.no_grad():\n",
    "        _input_token_count = _inputs_to_mask.shape[-1]\n",
    "        _batch_sz = _inputs_to_mask.shape[0]\n",
    "        _perm_sz = int(_input_token_count * _proportion_to_mask)\n",
    "        _permutation = np.random.permutation(np.arange(_input_token_count))\n",
    "        _masked_inputs = _inputs_to_mask.clone().detach()\n",
    "        _masked_attention_masks = torch.ones((_batch_sz, _input_token_count), dtype=torch.float)\n",
    "        _masked_token_types = torch.ones((_batch_sz, _input_token_count), dtype=torch.long)\n",
    "        for b in range(_batch_sz):\n",
    "            _masked_inputs[b][_permutation[0:_perm_sz]] = _tokenizer.mask_token_id\n",
    "            _masked_attention_masks[b][_permutation[0:_perm_sz]] = 0 # Don't pay attention to masked tokens.\n",
    "    return _masked_inputs, _masked_attention_masks, _masked_token_types\n",
    "\n",
    "\n",
    "def prepare_chunked_inputs(_batched_inputs, _max_seq_len, _max_title_len):\n",
    "    # We need to do a lot more data preparation before feeding into the model.\n",
    "    # First, chunk the batch into a list of tensors each of size max_seq_len.\n",
    "    with torch.no_grad():\n",
    "        _batch_sz = _batched_inputs[0].shape[0]\n",
    "        _nr_chunks = int(_batched_inputs[0].shape[-1] / _max_seq_len)\n",
    "        _batch_inputs = _batched_inputs[0:3]\n",
    "        _chunked_batch_inputs_by_input_nr = [torch.chunk(_bi, _nr_chunks, -1) for _bi in _batch_inputs]\n",
    "        _chunked_batch_inputs = []\n",
    "        \n",
    "        # Next, gather the expected output IDs, which will be filled with masks and appended on to the input tensors.\n",
    "        _labels = _batched_inputs[3]\n",
    "        \n",
    "        for i in range(_nr_chunks):\n",
    "            _input_mask_tensor, _ones_float_tensor_for_title, _ones_long_tensor_for_title = random_mask(_labels, tokenizer, .5)\n",
    "            \n",
    "            # For the input_ids (index 0), append on _max_title_len masks.\n",
    "            _chunked_input_ids = torch.cat([_chunked_batch_inputs_by_input_nr[0][i], _input_mask_tensor], dim=-1)\n",
    "            # For the attention mask, just add all 1s because this is not padding.\n",
    "            _chunked_attention_mask = torch.cat([_chunked_batch_inputs_by_input_nr[1][i], _ones_float_tensor_for_title], dim=-1)\n",
    "            # For token type IDs, also all 1s since this is the \"second sentence\".\n",
    "            _chunked_token_type_ids = torch.cat([_chunked_batch_inputs_by_input_nr[2][i], _ones_long_tensor_for_title], dim=-1)\n",
    "            _chunked_batch_inputs.append([_chunked_input_ids, _chunked_attention_mask, _chunked_token_type_ids])\n",
    "        \n",
    "        # Create a target mapping that will be used for all inputs, since they all follow a similar format.\n",
    "        _target_mapping = torch.zeros((_batch_sz, _max_title_len, _max_seq_len + _max_title_len), dtype=torch.float)\n",
    "        for i in range(_max_title_len):\n",
    "            for b in range(_batch_sz):\n",
    "                _target_mapping[b][i][_max_seq_len + i] = 1\n",
    "        \n",
    "    return _chunked_batch_inputs, _target_mapping, _labels\n",
    "\n",
    "def chunk_to_inputs(_chunk, _target_mapping, _labels, _mems, _device):\n",
    "    _inputs = {\"input_ids\": _chunk[0], \n",
    "              \"attention_mask\": _chunk[1], \n",
    "              \"token_type_ids\": _chunk[2],\n",
    "              \"target_mapping\": _target_mapping}\n",
    "\n",
    "    if _labels is not None:\n",
    "        _inputs[\"labels\"] = _labels\n",
    "\n",
    "    # Don't forget to send all these tensors to the device.\n",
    "    __s = time.time()\n",
    "    for i, (k,v) in enumerate(_inputs.items()):\n",
    "        _inputs[k] = v.to(_device)\n",
    "    xfer_times.append(time.time() - __s)\n",
    "    \n",
    "    # Mems will just stay on-device, so add them last.\n",
    "    if _mems is not None:\n",
    "        _inputs[\"mems\"] = _mems\n",
    "    return _inputs\n",
    "\n",
    "def train_epoch(_model, _optimizer, _scheduler, _device, _dataloader, _max_seq_len, _max_title_len, _fp16):\n",
    "    clear_timers()\n",
    "    \n",
    "    _epoch_iterator = tqdm(_dataloader, desc=\"Iteration\")\n",
    "    _steps = 0\n",
    "    _tr_loss, _logging_loss = 0, 0\n",
    "    _accuracy_accum, _accuracy_last = 0, 0\n",
    "    _model.train()\n",
    "    \n",
    "    for _step, _batch in enumerate(_epoch_iterator):\n",
    "        __s = time.time()\n",
    "        _chunked_batch_inputs, _target_mapping, _labels = prepare_chunked_inputs(_batch, _max_seq_len, _max_title_len)\n",
    "        _num_chunks = len(_chunked_batch_inputs)\n",
    "        preprocess_times.append(time.time() - __s)\n",
    "        \n",
    "        _mems = None\n",
    "        _loss = None\n",
    "        _chunk_loss_schedule = []\n",
    "        for _chunk in _chunked_batch_inputs:\n",
    "            _inputs = chunk_to_inputs(_chunk, _target_mapping, _labels, _mems, _device)\n",
    "            \n",
    "            # Forward\n",
    "            __s = time.time()\n",
    "            _loss, _logits, _mems = _model.forward(**_inputs)\n",
    "            forward_times.append(time.time() - __s)            \n",
    "            \n",
    "            # Backwards\n",
    "            __s = time.time()\n",
    "            if fp16:\n",
    "                with amp.scale_loss(_loss, _optimizer) as _scaled_loss:\n",
    "                    _scaled_loss.backward()\n",
    "                    backward_time = time.time() - __s\n",
    "            else:\n",
    "                _loss.backward()\n",
    "                backward_time = time.time() - __s\n",
    "            backward_times.append(backward_time)\n",
    "            \n",
    "            # Update weights\n",
    "            if _fp16:\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(_optimizer), 1)\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(_model.parameters(), 1)\n",
    "            __s = time.time()\n",
    "            _optimizer.step()\n",
    "            opt_times.append(time.time() - __s)\n",
    "            __s = time.time()\n",
    "            _scheduler.step()\n",
    "            sched_times.append(time.time() - __s)\n",
    "            _model.zero_grad()\n",
    "            \n",
    "            _chunk_loss_schedule.append(_loss.item())\n",
    "        \n",
    "        # Always accumulate loss across the last chunk, where it should be lowest. That's the goal of this model.\n",
    "        _steps += 1\n",
    "        _tr_loss += _loss.item()\n",
    "        \n",
    "        # Always log.\n",
    "        _loss_scalar = (_tr_loss - _logging_loss)\n",
    "        _logging_loss = _tr_loss\n",
    "        _logs = {}\n",
    "        _logs[\"loss_\" + str(_num_chunks)] = _loss_scalar\n",
    "        _logs[\"learning_rate\"] = _scheduler.get_lr()[0]\n",
    "        if do_wandb:\n",
    "            wandb.log(_logs)\n",
    "\n",
    "\n",
    "def validate_epoch(_model, _device, _dataloader, _max_seq_len, _max_title_len):\n",
    "    _epoch_iterator = tqdm(_dataloader, desc=\"Iteration\")\n",
    "    _steps = 0\n",
    "    _tr_loss, _logging_loss = 0, 0\n",
    "    _accuracy_accum, _accuracy_last = 0, 0\n",
    "    _model.train()\n",
    "    for _step, _batch in enumerate(_epoch_iterator):\n",
    "        _chunked_batch_inputs, _target_mapping, _labels = prepare_chunked_inputs(_batch, _max_seq_len, _max_title_len)\n",
    "        _num_chunks = len(_chunked_batch_inputs)\n",
    "        \n",
    "        _mems = None\n",
    "        _loss = None\n",
    "        _chunk_loss_schedule = []\n",
    "        for _chunk in _chunked_batch_inputs:\n",
    "            _inputs = chunk_to_inputs(_chunk, _target_mapping, _labels, _mems, _device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _loss, _logits, _mems = _model.forward(**_inputs)\n",
    "            \n",
    "            _chunk_loss_schedule.append(_loss.item())\n",
    "        \n",
    "        # Always accumulate loss across the last chunk, where it should be lowest. That's the goal of this model.\n",
    "        _steps += 1\n",
    "        _tr_loss += _loss.item()\n",
    "        break\n",
    "    return _tr_loss, _steps\n",
    "\n",
    "print(\"***** Running training *****\")\n",
    "\n",
    "def full_validate():\n",
    "    combined_val_steps, combined_val_loss = 0, 0\n",
    "    for i, val_dataset in enumerate(val_datasets):\n",
    "        print(\"Running validation %i..\" % (i))\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "        l, s = validate_epoch(model, device, val_dataloader, CHUNK_SEQ_LEN, TITLE_PRED_MAX_LEN)\n",
    "        combined_val_steps += s\n",
    "        combined_val_loss += l\n",
    "        \n",
    "    _logs = {}\n",
    "    _logs[\"val_loss\"] = combined_val_loss / combined_val_steps\n",
    "    if do_wandb:\n",
    "        wandb.log(_logs)\n",
    "    print(\"Validation loss averaged over %i steps: %f\" % (int(combined_val_steps), combined_val_loss / combined_val_steps))\n",
    "\n",
    "model.zero_grad()\n",
    "for _ in range(EPOCHS):\n",
    "    random.shuffle(train_datasets)\n",
    "    for train_dataset in train_datasets:\n",
    "        full_validate()\n",
    "        train_sampler = RandomSampler(train_dataset)\n",
    "        train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        train_epoch(model, optimizer, scheduler, device, train_dataloader, CHUNK_SEQ_LEN, TITLE_PRED_MAX_LEN, fp16)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Save the model \n",
    "output_dir = os.path.join(\"c:/Users/jbetk/Documents/data/ml/saved_models\", \"xlnet_title_generation\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "model_to_save = (\n",
    "    model.module if hasattr(model, \"module\") else model\n",
    ")  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"model.pt\"))\n",
    "torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "\n",
    "print(\"Save completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the model.\n",
    "actual_article_title = \"Italy announces lockdown as global coronavirus cases surpass 105,000\"\n",
    "article_text = \"\"\"\n",
    "Italian Prime Minister Giuseppe Conte signed a decree early Sunday that will put millions of people across northern Italy under lockdown due to the novel coronavirus.\n",
    "The sweeping move puts the entire Lombardy region, as well as 14 other provinces, under travel restrictions, and is one of the toughest responses implemented outside of mainland China to get the Covid-19 epidemic under control.\n",
    "CNN is verifying exactly when the lockdown will go into effect.\n",
    "The announcement came after Italy saw a dramatic spike of 1,247 confirmed novel coronavirus cases on Saturday, the Civil Protection Department said in a statement.\n",
    "The country has now recorded 5,883 cases and 233 deaths, the most fatalities outside mainland China and the biggest outbreak in Europe.\n",
    "Announcing the new measures, Conte said: \"There will be an obligation to avoid any movement of people who are either entering or leaving\" the affected areas. \"Even within the areas moving around will occur only for essential work or health reasons,\" he said, according to Reuters.\n",
    "While the lockdown only applies to northern Italy, other measures will be applied to the entire country. These include the suspension of schools, university classes, theaters and cinemas, as well as bars, nightclubs, and sports events. Religious ceremonies, including funerals, will also be suspended.\n",
    "Other countries in Europe are also struggling to contain outbreaks as cases continue to rise.\n",
    "On Saturday, France's general director of health, Jerome Salomon, confirmed 16 dead and 949 infected nationwide, and Germany now has 795 cases. The United Kingdom confirmed a second death from the novel coronavirus on Saturday, while 206 people have tested positive, British health officials said in a statement.\n",
    "The World Health Organization (WHO) has called on \"all countries to continue efforts that have been effective in limiting the number of cases and slowing the spread of the virus.\"\n",
    "In a statement, the WHO said: \"Allowing uncontrolled spread should not be a choice of any government, as it will harm not only the citizens of that country but affect other countries as well.\"\n",
    "Meanwhile in China, search and rescue efforts continued on Sunday for survivors from the collapse of a hotel that was being used as a coronavirus quarantine center.\n",
    "The hotel, in the southeastern city of Quanzhou, in Fujian province, came down Saturday night with 80 people inside. Four people died, one person remains in critical condition and four others are seriously injured, according to China's Ministry of Emergency Management.\n",
    "\"We are using life detection instruments to monitor signs of life and professional breaking-in tools to make forcible entries. We are trying our utmost to save trapped people,\" said Guo Yutuan, squadron leader of the Quanzhou armed police detachment's mobile unit.\n",
    "The building's owner is in police custody, according to state news agency Xinhua and an investigation is underway.\n",
    "\"\"\"\n",
    "\n",
    "def test_model(_text_input, _test_model, _seq_len, _title_len, _test_device):\n",
    "    tokenized_text_plus = tokenizer.encode_plus(_text_input, add_special_tokens=True, max_length=None, pad_to_max_length=False,\n",
    "                                   return_token_type_ids=True, return_attention_mask=True)\n",
    "    # The chunker expects a labels element, but we dont actually want to supply one for test; just supply an empty tensor.\n",
    "    tokenized_text_plus.append(torch.empty((0,), dtype=torch.long))\n",
    "    _test_batch = [tokenized_text_plus]\n",
    "    \n",
    "    _test_chunked_batch_inputs, _test_target_mapping, _test_labels = prepare_chunked_inputs(_test_batch, _seq_len, _title_len)\n",
    "    _test_num_chunks = len(_test_chunked_batch_inputs)\n",
    "    \n",
    "    _test_mems = None\n",
    "    _test_loss = None\n",
    "    _test_chunk_loss_schedule = []\n",
    "    _test_logits = None\n",
    "    with torch.no_grad():\n",
    "        for _test_chunk in _test_chunked_batch_inputs:\n",
    "            _test_inputs = chunk_to_inputs(_test_chunk, _test_target_mapping, None, _test_mems, _test_device)\n",
    "            # I'm assuming no loss is returned since I'm not giving it any labels - may need to adjust this.\n",
    "            _test_logits, _test_mems = _model.forward(**_test_inputs)\n",
    "            _test_chunk_loss_schedule.append(_test_loss.item())\n",
    "        _test_logits_argmax = torch.argmax(_test_logits, dim=-1)\n",
    "        return tokenizer.decode(_test_logits_argmax[0].numpy())\n",
    "    \n",
    "print(test_model(article_text, model, CHUNK_SEQ_LEN, TITLE_PRED_MAX_LEN, device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}