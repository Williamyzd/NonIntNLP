{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "from transformers import glue_output_modes as output_modes\n",
    "from transformers import glue_processors as processors\n",
    "\n",
    "fp16 = True\n",
    "if fp16:\n",
    "    from apex import amp\n",
    "\n",
    "model_name = \"albert-large-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "tokenizer = transformers.AlbertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def load_dataset(examples, processor):\n",
    "    output_mode = output_modes[task]\n",
    "    label_list = processor.get_labels()\n",
    "    features = convert_examples_to_features(examples, \n",
    "                                            tokenizer, \n",
    "                                            label_list=label_list,\n",
    "                                            max_length=128,\n",
    "                                            pad_on_left=False,\n",
    "                                            output_mode=output_mode,\n",
    "                                            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "                                            pad_token_segment_id=0)\n",
    "    input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "    labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "    return TensorDataset(input_ids, attention_mask, token_type_ids, labels)\n",
    "\n",
    "# Process dataset\n",
    "task = 'mrpc'\n",
    "input_file = \"C:\\\\Users\\\\jbetk\\\\Documents\\\\data\\\\ml\\\\text_similarity\\\\MSRParaphraseCorpus\"\n",
    "processor = processors[task]()\n",
    "train_dataset = load_dataset(processor.get_train_examples(input_file), processor)\n",
    "val_dataset = load_dataset(processor.get_dev_examples(input_file), processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Load model\n",
    "config = transformers.AlbertConfig.from_pretrained(model_name)\n",
    "model = transformers.AlbertForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "device = torch.device(\"cuda\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0,\n",
    "    },\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "#print(\"Parameters: \", optimizer_grouped_parameters)\n",
    "optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
    "                                                         num_warmup_steps=0, num_training_steps=len(train_dataset))\n",
    "\n",
    "# Shift model to cuda & enable fp16 if applicable.\n",
    "model.to(device)\n",
    "if fp16:\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "    \n",
    "# Initialize w&b logger\n",
    "do_wandb = False\n",
    "if do_wandb:\n",
    "    wandb.init(project=\"nonint-transformers-torch\",\\\n",
    "               name=\"albert_sem_comp_msrpc\",\\\n",
    "               config={\"dataset\": \"msrpc\"})\n",
    "    # There's something bugged about this, but it doesnt really seem to do much anyways. Apparently it enables some \n",
    "    # sort of gradient exploration map.\n",
    "    #wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "xfer_times = []\n",
    "forward_times = []\n",
    "backward_times = []\n",
    "opt_times = []\n",
    "sched_times = []\n",
    "\n",
    "def compute_accuracy(_pred, _true):\n",
    "    return np.sum(_pred == _true) / _pred.shape[0]\n",
    "\n",
    "def clear_timers():\n",
    "    xfer_times.clear()\n",
    "    forward_times.clear()\n",
    "    backward_times.clear()\n",
    "    opt_times.clear()\n",
    "    sched_times.clear()\n",
    "\n",
    "def train_epoch(_model, _optimizer, _scheduler, _device, _dataloader, _logging_steps, _fp16=False):\n",
    "    clear_timers()\n",
    "    \n",
    "    _epoch_iterator = tqdm(_dataloader, desc=\"Iteration\")\n",
    "    _steps = 0\n",
    "    _tr_loss, _logging_loss = 0, 0\n",
    "    _accuracy_accum, _accuracy_last = 0, 0\n",
    "    _model.train()\n",
    "    for _step, _batch in enumerate(_epoch_iterator):\n",
    "        __s = time.time()\n",
    "        _batch = tuple(_t.to(_device) for _t in _batch)\n",
    "        _inputs = {\"input_ids\": _batch[0], \n",
    "                   \"attention_mask\": _batch[1], \n",
    "                   \"token_type_ids\": _batch[2], \n",
    "                   \"labels\": _batch[3]}\n",
    "        xfer_times.append(time.time() - __s)\n",
    "        \n",
    "        __s = time.time()\n",
    "        _outputs = _model(**_inputs)\n",
    "        forward_times.append(time.time() - __s)\n",
    "        \n",
    "        _loss = _outputs[0]\n",
    "        \n",
    "        backward_time = 0\n",
    "        __s = time.time()\n",
    "        if fp16:\n",
    "            with amp.scale_loss(_loss, _optimizer) as _scaled_loss:\n",
    "                _scaled_loss.backward()\n",
    "                backward_time = time.time() - __s\n",
    "        else:\n",
    "            _loss.backward()\n",
    "            backward_time = time.time() - __s\n",
    "        backward_times.append(backward_time)\n",
    "        \n",
    "        _tr_loss += _loss.item()\n",
    "        _accuracy_accum += compute_accuracy(np.argmax(_outputs[1].detach().cpu().numpy(), axis=-1), _batch[3].cpu().numpy())\n",
    "        \n",
    "        if _fp16:\n",
    "            torch.nn.utils.clip_grad_norm_(amp.master_params(_optimizer), 1)\n",
    "        else:\n",
    "            torch.nn.utils.clip_grad_norm_(_model.parameters(), 1)\n",
    "        \n",
    "        __s = time.time()\n",
    "        _optimizer.step()\n",
    "        opt_times.append(time.time() - __s)\n",
    "        __s = time.time()\n",
    "        _scheduler.step()\n",
    "        sched_times.append(time.time() - __s)\n",
    "        _model.zero_grad()\n",
    "        _steps += 1\n",
    "        \n",
    "        # Log\n",
    "        if _steps % _logging_steps == 0:\n",
    "            _loss_scalar = (_tr_loss - _logging_loss) / _logging_steps\n",
    "            _accuracy_scalar = (_accuracy_accum - _accuracy_last) / _logging_steps\n",
    "            _logging_loss = _tr_loss\n",
    "            _accuracy_last = _accuracy_accum\n",
    "            _logs = {}\n",
    "            _logs[\"loss\"] = _loss_scalar\n",
    "            _logs[\"accuracy\"] = _accuracy_scalar\n",
    "            _logs[\"learning_rate\"] = _scheduler.get_lr()[0]\n",
    "            #print(json.dumps({**_logs, **{\"step\": _steps}}))\n",
    "            if do_wandb:\n",
    "                wandb.log(_logs)\n",
    "    \n",
    "def check_validation(_model, _device, _val_dataloader):\n",
    "    with torch.no_grad():\n",
    "        _val_iterator = tqdm(_val_dataloader, desc=\"Validation iteration\")\n",
    "        _loss = 0\n",
    "        _accuracy = 0\n",
    "        for _step, _batch in enumerate(_val_iterator):\n",
    "            _batch = tuple(_t.to(device) for _t in _batch)\n",
    "            _inputs = {\"input_ids\": _batch[0], \n",
    "                       \"attention_mask\": _batch[1], \n",
    "                       \"token_type_ids\": _batch[2], \n",
    "                       \"labels\": _batch[3]}\n",
    "            _outputs = model(**_inputs)\n",
    "            _loss += _outputs[0].item()\n",
    "            _accuracy += compute_accuracy(np.argmax(_outputs[1].cpu().numpy(), axis=-1), _batch[3].cpu().numpy())\n",
    "        _loss_computed = _loss/len(_val_dataloader)\n",
    "        _acc_computed = _accuracy/len(_val_dataloader)\n",
    "        print(\"Validation loss %f, accuracy=%f\" % (_loss_computed, _acc_computed))\n",
    "        if do_wandb:\n",
    "            wandb.log({'val_loss': _loss_computed, 'val_accuracy': _acc_computed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 4076\n",
      "  Num Epochs = 1\n",
      "  Total optimization steps = 4076\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "\n",
      "\n",
      "Validation loss 0.377003, accuracy=0.834987\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=170.0, style=ProgressStyle(description_wi…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8944d4614fd49c0a809d51933e17a76"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": [
      "c:\\drive\\projects\\ml-notebooks\\pytorch-venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Validation iteration', max=72.0, style=ProgressStyle(desc…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db77c79de712476ea4f60f2fe38792ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LOGGING_STEPS = 5\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 24\n",
    "\n",
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\" % (len(train_dataset)))\n",
    "print(\"  Num Epochs = %d\" % (EPOCHS))\n",
    "print(\"  Total optimization steps = %d\" % (len(train_dataset)))\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "model.zero_grad()\n",
    "for _ in range(EPOCHS):\n",
    "    train_epoch(model, optimizer, scheduler, device, train_dataloader, LOGGING_STEPS, _fp16=True)\n",
    "    check_validation(model, device, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def infer(_model, _sentence1, _sentence2):\n",
    "    features = [tokenizer.encode_plus(text=_sentence1, text_pair=_sentence2, max_length=128, pad_to_max_length=True)]\n",
    "    _inputs = {\"input_ids\": torch.tensor([f['input_ids'] for f in features], dtype=torch.long).to(device), \n",
    "               \"attention_mask\": torch.tensor([f['attention_mask'] for f in features], dtype=torch.long).to(device), \n",
    "               \"token_type_ids\": torch.tensor([f['token_type_ids'] for f in features], dtype=torch.long).to(device)}\n",
    "    with torch.no_grad():\n",
    "        _outputs = model(**_inputs)\n",
    "        return softmax(_outputs[0].cpu().numpy())\n",
    "    \n",
    "print(infer(model, \"The man and the woman went to the store\", \"The woman and the man went to the store\"))\n",
    "print(infer(model, \"The man and the woman went to the store\", \"The man walked with his wife\"))\n",
    "print(infer(model, \"The man and the woman went to the store\", \"The man trudged along with his daughter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save the model \n",
    "output_dir = os.path.join(\"c:/Users/jbetk/Documents/data/ml/saved_models\", \"semantic_comparison_pytorch\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "model_to_save = (\n",
    "    model.module if hasattr(model, \"module\") else model\n",
    ")  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"model.pt\"))\n",
    "torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save to torchscript\n",
    "dummy_input = [\n",
    "    torch.zeros(1, 128, dtype=torch.long),\n",
    "    torch.zeros(1, 128, dtype=torch.long),\n",
    "    torch.zeros(1, 128, dtype=torch.long),\n",
    "]\n",
    "__config = transformers.AlbertConfig.from_pretrained(output_dir, torchscript=True)\n",
    "__model = transformers.AlbertForSequenceClassification.from_pretrained(output_dir, config=__config)\n",
    "__model.eval()\n",
    "#model(*dummy_input)\n",
    "traced_model = torch.jit.trace(__model, dummy_input)\n",
    "torch.jit.save(traced_model, os.path.join(output_dir, \"torchscript_out.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save the model \n",
    "output_dir = os.path.join(\"c:/Users/jbetk/Documents/data/ml/saved_models\", \"semantic_comparison_pytorch\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "model_to_save = (\n",
    "    model.module if hasattr(model, \"module\") else model\n",
    ")  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"model.pt\"))\n",
    "torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Save to torchscript\n",
    "dummy_input = [\n",
    "    torch.zeros(1, 128, dtype=torch.long),\n",
    "    torch.zeros(1, 128, dtype=torch.long),\n",
    "    torch.zeros(1, 128, dtype=torch.long),\n",
    "]\n",
    "__config = transformers.AlbertConfig.from_pretrained(output_dir, torchscript=True)\n",
    "__model = transformers.AlbertForSequenceClassification.from_pretrained(output_dir, config=__config)\n",
    "__model.eval()\n",
    "#model(*dummy_input)\n",
    "traced_model = torch.jit.trace(__model, dummy_input)\n",
    "torch.jit.save(traced_model, os.path.join(output_dir, \"torchscript_out.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}