{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "\n",
    "from chunked_text_dataloader import ChunkedTextDataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "fp16 = True\n",
    "if fp16:\n",
    "    from apex import amp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/neonbjb/nonint-transformers-torch\" target=\"_blank\">https://app.wandb.ai/neonbjb/nonint-transformers-torch</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/neonbjb/nonint-transformers-torch/runs/z0jhvs4q\" target=\"_blank\">https://app.wandb.ai/neonbjb/nonint-transformers-torch/runs/z0jhvs4q</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.29 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 4\n",
    "CHUNK_SEQ_LEN = 256\n",
    "TITLE_PRED_MAX_LEN = 32\n",
    "model_name = \"xlnet-base-cased\"\n",
    "\n",
    "tokenizer = transformers.XLNetTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Get the datasets\n",
    "input_folder = \"C:\\\\Users\\\\jbetk\\\\Documents\\\\data\\\\ml\\\\title_prediction\\\\outputs\\\\\"\n",
    "train_set = ChunkedTextDataset(input_folder + \"train.pt\", tokenizer, CHUNK_SEQ_LEN, TITLE_PRED_MAX_LEN, mask_target_percentage=.5,\n",
    "                               pad_left=True, force_max_len_gen=False)\n",
    "val_set = ChunkedTextDataset(input_folder + \"val.pt\", tokenizer, CHUNK_SEQ_LEN, TITLE_PRED_MAX_LEN, mask_target_percentage=.5,\n",
    "                               pad_left=True, force_max_len_gen=False)\n",
    "test_set = ChunkedTextDataset(input_folder + \"test.pt\", tokenizer, CHUNK_SEQ_LEN, TITLE_PRED_MAX_LEN, mask_target_percentage=.5,\n",
    "                               pad_left=True, force_max_len_gen=False)\n",
    "train_loader = train_set.get_dataloader(BATCH_SIZE, num_workers=0)\n",
    "val_loader = val_set.get_dataloader(BATCH_SIZE, num_workers=0, random=False)\n",
    "test_loader = test_set.get_dataloader(BATCH_SIZE, num_workers=0, random=False)\n",
    "\n",
    "# Initialize w&b logger\n",
    "do_wandb = True\n",
    "if do_wandb:\n",
    "    wandb.init(project=\"nonint-transformers-torch\",\\\n",
    "               name=\"xlnet_title_prediction_front_unfixed_maskes_256_seq\",\\\n",
    "               config={\"dataset\": \"title_pred\"})\n",
    "    # There's something bugged about this, but it doesnt really seem to do much anyways. Apparently it enables some \n",
    "    # sort of gradient exploration map.\n",
    "    #wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "config = transformers.XLNetConfig.from_pretrained(model_name)\n",
    "config.mem_len = 1024\n",
    "model = transformers.XLNetLMHeadModel.from_pretrained(model_name, config=config)\n",
    "device = torch.device(\"cuda\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0,\n",
    "    },\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
    "                                                         num_warmup_steps=0, num_training_steps=EPOCHS * len(train_set))\n",
    "\n",
    "# Shift model to cuda & enable fp16 if applicable.\n",
    "model.to(device)\n",
    "if fp16:\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe1b08262db484588c9484fb9260da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=24894.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\drive\\projects\\ml-notebooks\\pytorch-venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\drive\\projects\\ml-notebooks\\pytorch-venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "wandb: Wandb version 0.8.29 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save completed. c:/Users/jbetk/Documents/data/ml/saved_models\\xlnet_title_generation\\chkpt_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0195ebbd81ff433fbb2ea777aef3dfd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validation Iteration', max=512.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 7.229584409640386\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "Save completed. c:/Users/jbetk/Documents/data/ml/saved_models\\xlnet_title_generation\\chkpt_2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eead831295724f5583a623fd6497d7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validation Iteration', max=512.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 1.9668402855212872\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"
     ]
    }
   ],
   "source": [
    "preprocess_times = []\n",
    "forward_times = []\n",
    "backward_times = []\n",
    "opt_times = []\n",
    "\n",
    "def clear_timers():\n",
    "    forward_times.clear()\n",
    "    backward_times.clear()\n",
    "    opt_times.clear()\n",
    "\n",
    "def save_model(_model, _chkpt_name):\n",
    "    # Save the model \n",
    "    _output_dir = os.path.join(\"c:/Users/jbetk/Documents/data/ml/saved_models\", \"xlnet_title_generation\", _chkpt_name)\n",
    "    if not os.path.exists(_output_dir):\n",
    "        os.makedirs(_output_dir)\n",
    "    _model_to_save = (\n",
    "        _model.module if hasattr(_model, \"module\") else _model\n",
    "    )  # Take care of distributed/parallel training\n",
    "    _model_to_save.save_pretrained(_output_dir)\n",
    "    print(\"Save completed. %s\" % (_output_dir))\n",
    "\n",
    "def train_epoch(_model, _optimizer, _scheduler, _device, _dataloader, _max_seq_len, _max_title_len, _fp16):\n",
    "    _logging_steps = 5\n",
    "    _steps_till_save = 2000\n",
    "    _steps_till_validate = 2000\n",
    "    \n",
    "    clear_timers()\n",
    "    \n",
    "    _epoch_iterator = tqdm(_dataloader, desc=\"Iteration\")\n",
    "    _steps = 0\n",
    "    _tr_loss, _logging_loss = 0, 0\n",
    "    _chunks = 0\n",
    "    _accuracy_accum, _accuracy_last = 0, 0\n",
    "    _model.train()\n",
    "    \n",
    "    __s = time.time()\n",
    "    for _step, _batch in enumerate(_epoch_iterator):\n",
    "        preprocess_times.append(time.time() - __s)\n",
    "        \n",
    "        _mems = None\n",
    "        _loss = None\n",
    "        _chunk_loss_schedule = []\n",
    "        _num_chunks = len(_batch[\"input_ids\"])\n",
    "        _chunks += _num_chunks\n",
    "        for _masked_input_ids, _attention_masks, _labels in zip(_batch['input_ids_masked'],\n",
    "                                                                _batch['attention_masks'],\n",
    "                                                                _batch['labels']):            \n",
    "            # Forward\n",
    "            _inputs = {\n",
    "                'input_ids': _masked_input_ids.to(_device),\n",
    "                'attention_mask': _attention_masks.to(_device),\n",
    "                'labels': _labels.to(_device)\n",
    "            }\n",
    "            if _mems is not None:\n",
    "                _inputs['mems'] = _mems\n",
    "            \n",
    "            __s = time.time()\n",
    "            _loss, _logits, _mems = _model.forward(**_inputs)\n",
    "            forward_times.append(time.time() - __s)            \n",
    "            \n",
    "            # Backwards\n",
    "            __s = time.time()\n",
    "            if fp16:\n",
    "                with amp.scale_loss(_loss, _optimizer) as _scaled_loss:\n",
    "                    _scaled_loss.backward()\n",
    "                    backward_time = time.time() - __s\n",
    "            else:\n",
    "                _loss.backward()\n",
    "                backward_time = time.time() - __s\n",
    "            backward_times.append(backward_time)\n",
    "            \n",
    "            # Update weights\n",
    "            if _fp16:\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(_optimizer), 1)\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(_model.parameters(), 1)\n",
    "            __s = time.time()\n",
    "            _optimizer.step()\n",
    "            opt_times.append(time.time() - __s)\n",
    "            _scheduler.step()\n",
    "            _model.zero_grad()\n",
    "            \n",
    "            _chunk_loss_schedule.append(_loss.item())\n",
    "        \n",
    "        # Always accumulate loss across the last chunk, where it should be lowest. That's the goal of this model.\n",
    "        _tr_loss += _loss.item()\n",
    "        \n",
    "        if _steps % _logging_steps == 0:\n",
    "            _loss_scalar = (_tr_loss - _logging_loss) / _logging_steps\n",
    "            _logging_loss = _tr_loss\n",
    "            _logs = {}\n",
    "            _logs[\"avg_chunks\"] = _chunks / _logging_steps\n",
    "            _chunks = 0\n",
    "            _logs[\"loss\"] = _loss_scalar\n",
    "            _logs[\"learning_rate\"] = _scheduler.get_lr()[0]\n",
    "            if do_wandb:\n",
    "                wandb.log(_logs)\n",
    "        \n",
    "        if _steps % _steps_till_save == 0:\n",
    "            save_model(model, \"chkpt_%i\" % (_steps))\n",
    "        if _steps % _steps_till_validate == 0:\n",
    "            validate(_model, _device, _max_seq_len, _max_title_len)\n",
    "            \n",
    "        _steps += 1\n",
    "        # Record time so we see how long it takes to fetch a batch.\n",
    "        __s = time.time()\n",
    "\n",
    "\n",
    "def validate(_model, _device, _max_seq_len, _max_title_len):\n",
    "    _epoch_iterator = tqdm(val_loader, desc=\"Validation Iteration\")\n",
    "    _actual_steps = 0\n",
    "    _total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _step, _batch in enumerate(_epoch_iterator):\n",
    "            # Skip 1 in 10 steps, because the validator just takes too long otherwise. It's not as easy as just cutting\n",
    "            # down the dataset, either, since we run into chunk/batch size mismatches then.\n",
    "            if _step % 10 != 0:\n",
    "                continue\n",
    "            _mems = None\n",
    "            _loss = None\n",
    "            for _masked_input_ids, _attention_masks, _labels in zip(_batch['input_ids_masked'],\n",
    "                                                                    _batch['attention_masks'],\n",
    "                                                                    _batch['labels']):            \n",
    "                # Forward\n",
    "                _inputs = {\n",
    "                    'input_ids': _masked_input_ids.to(_device),\n",
    "                    'attention_mask': _attention_masks.to(_device),\n",
    "                    'labels': _labels.to(_device)\n",
    "                }\n",
    "                if _mems is not None:\n",
    "                    _inputs['mems'] = _mems\n",
    "                \n",
    "                _loss, _logits, _mems = _model.forward(**_inputs)\n",
    "            \n",
    "            # Always accumulate loss across the last chunk, where it should be lowest. That's the goal of this model.\n",
    "            _actual_steps += 1\n",
    "            _total_loss += _loss.item()\n",
    "    \n",
    "        _logs = {}\n",
    "        _val_loss = _total_loss / _actual_steps\n",
    "        _logs[\"val_loss\"] = _val_loss\n",
    "        if do_wandb:\n",
    "            wandb.log(_logs)\n",
    "        print(\"Validation loss: \" + str(_val_loss))\n",
    "\n",
    "print(\"***** Running training *****\")\n",
    "\n",
    "model.zero_grad()\n",
    "for _ in range(EPOCHS):    \n",
    "    train_epoch(model, optimizer, scheduler, device, train_loader, CHUNK_SEQ_LEN, TITLE_PRED_MAX_LEN, fp16)\n",
    "    # Slowly increase the mask percentage per epoch to make the model have to work harder.\n",
    "    train_set.mask_target_percentage += .1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model, device, CHUNK_SEQ_LEN, TITLE_PRED_MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = os.path.join(\"c:/Users/jbetk/Documents/data/ml/saved_models\", \"xlnet_title_generation\")\n",
    "\n",
    "# Load model from saved state.\n",
    "config = transformers.XLNetConfig.from_pretrained(output_dir)\n",
    "config.mem_len = 1024\n",
    "model = transformers.XLNetLMHeadModel.from_pretrained(output_dir, config=config)\n",
    "device = torch.device(\"cuda\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "#optimizer = torch.load(os.path.join(output_dir, \"optimizer.pt\"))\n",
    "#scheduler = torch.load(os.path.join(output_dir, \"scheduler.pt\"))\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0,\n",
    "    },\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
    "                                                         num_warmup_steps=0, num_training_steps=EPOCHS * len(train_set))\n",
    "\n",
    "# Shift model to cuda & enable fp16 if applicable.\n",
    "model.to(device)\n",
    "if fp16:\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the model.\n",
    "actual_article_title = \"Italy announces lockdown as global coronavirus cases surpass 105,000\"\n",
    "article_text = \"\"\"\n",
    "Italian Prime Minister Giuseppe Conte signed a decree early Sunday that will put millions of people across northern Italy under lockdown due to the novel coronavirus.\n",
    "The sweeping move puts the entire Lombardy region, as well as 14 other provinces, under travel restrictions, and is one of the toughest responses implemented outside of mainland China to get the Covid-19 epidemic under control.\n",
    "CNN is verifying exactly when the lockdown will go into effect.\n",
    "The announcement came after Italy saw a dramatic spike of 1,247 confirmed novel coronavirus cases on Saturday, the Civil Protection Department said in a statement.\n",
    "The country has now recorded 5,883 cases and 233 deaths, the most fatalities outside mainland China and the biggest outbreak in Europe.\n",
    "Announcing the new measures, Conte said: \"There will be an obligation to avoid any movement of people who are either entering or leaving\" the affected areas. \"Even within the areas moving around will occur only for essential work or health reasons,\" he said, according to Reuters.\n",
    "While the lockdown only applies to northern Italy, other measures will be applied to the entire country. These include the suspension of schools, university classes, theaters and cinemas, as well as bars, nightclubs, and sports events. Religious ceremonies, including funerals, will also be suspended.\n",
    "Other countries in Europe are also struggling to contain outbreaks as cases continue to rise.\n",
    "On Saturday, France's general director of health, Jerome Salomon, confirmed 16 dead and 949 infected nationwide, and Germany now has 795 cases. The United Kingdom confirmed a second death from the novel coronavirus on Saturday, while 206 people have tested positive, British health officials said in a statement.\n",
    "The World Health Organization (WHO) has called on \"all countries to continue efforts that have been effective in limiting the number of cases and slowing the spread of the virus.\"\n",
    "In a statement, the WHO said: \"Allowing uncontrolled spread should not be a choice of any government, as it will harm not only the citizens of that country but affect other countries as well.\"\n",
    "Meanwhile in China, search and rescue efforts continued on Sunday for survivors from the collapse of a hotel that was being used as a coronavirus quarantine center.\n",
    "The hotel, in the southeastern city of Quanzhou, in Fujian province, came down Saturday night with 80 people inside. Four people died, one person remains in critical condition and four others are seriously injured, according to China's Ministry of Emergency Management.\n",
    "\"We are using life detection instruments to monitor signs of life and professional breaking-in tools to make forcible entries. We are trying our utmost to save trapped people,\" said Guo Yutuan, squadron leader of the Quanzhou armed police detachment's mobile unit.\n",
    "The building's owner is in police custody, according to state news agency Xinhua and an investigation is underway.\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "# Create inputs to the model for a given chunk of input_ids and a partially generated output.\n",
    "def create_inputs_for_chunk(_chunk: torch.Tensor, \n",
    "                            _mems: torch.Tensor, \n",
    "                            _tokenizer: transformers.PreTrainedTokenizer, \n",
    "                            _seq_len: int, \n",
    "                            _title_len: int, \n",
    "                            _test_device: torch.device):\n",
    "    assert(_outputs_so_far.shape[0] == _title_len)\n",
    "    _padding_needed = _seq_len - _chunk.shape[0] - _title_len\n",
    "    _pad_tensor = torch.full((_padding_needed,), _tokenizer.pad_token_id, dtype=torch.long)\n",
    "    \n",
    "    _input_ids = torch.cat([_pad_tensor, _chunk, _outputs_so_far]).unsqueeze(dim=0)\n",
    "    _attention_mask = torch.cat([torch.zeros((_padding_needed,), dtype=torch.float), torch.ones((_seq_len - _padding_needed), dtype=torch.float)]).unsqueeze(dim=0)\n",
    "    _token_type_ids = torch.cat([torch.zeros((_seq_len - _title_len), dtype=torch.long), torch.ones((_title_len,), dtype=torch.long)]).unsqueeze(dim=0)\n",
    "    _inputs = {\n",
    "        \"input_ids\": _input_ids.to(_test_device),\n",
    "        \"attention_mask\": _attention_mask.to(_test_device),\n",
    "        \"token_type_ids\": _token_type_ids.to(_test_device)\n",
    "    }\n",
    "    if _mems is not None:\n",
    "        _inputs[\"mems\"] = _mems\n",
    "    return _inputs\n",
    "\n",
    "# Returns top-k words the model predicts given _text_tensor and computed _outputs_so_far.\n",
    "def predict_words(_text_tensor: torch.Tensor, \n",
    "                  _outputs_so_far: torch.Tensor,\n",
    "                  _tokenizer: transformers.PreTrainedTokenizer, \n",
    "                  _test_model: transformers.PreTrainedModel, \n",
    "                  _seq_len: int, \n",
    "                  _title_len: int, \n",
    "                  _test_device: torch.device, \n",
    "                  _k_count=3):\n",
    "    _chunk_count = math.ceil(_text_tensor.shape[0] / (_seq_len - _title_len))\n",
    "    _tok_text_chunked = torch.chunk(_text_tensor, _chunk_count, dim=0)\n",
    "    _mems = None\n",
    "    for _chunk in _tok_text_chunked:\n",
    "        _inputs = create_inputs_for_chunk(_chunk, _mems, _tokenizer, _seq_len, _title_len, _test_device)\n",
    "        _logits, _mems = _test_model.forward(**_inputs)\n",
    "    # Remove the batch dimension.\n",
    "    _logits = _logits[0]\n",
    "    return torch.topk(_logits, _k_count)\n",
    "    \n",
    "def predict_forward(_text_tensor: torch.Tensor, \n",
    "                    _predict_tensor: torch.Tensor,\n",
    "                    _predict_index: int,\n",
    "                    _tokenizer: transformers.PreTrainedTokenizer, \n",
    "                    _test_model: transformers.PreTrainedModel, \n",
    "                    _seq_len: int, \n",
    "                    _title_len: int, \n",
    "                    _test_device: torch.device):\n",
    "    if _predict_index == _title_len:\n",
    "        return _predict_tensor\n",
    "    _words = predict_words(_text_tensor, _predict_tensor, _tokenizer, _test_model, _seq_len, _title_len, _test_device, 3)\n",
    "    for _prob, _word in _words:\n",
    "        print(\"Predict %s at %f probability\" % (_tokenizer.decode([_word]), _prob))\n",
    "    _predict_tensor[_predict_index] = _words[0][1]\n",
    "    if _predict_tensor[_predict_index] == _tokenizer.eos_token_id:\n",
    "        return _predict_tensor\n",
    "    return predict_forward(_text_tensor, _predict_tensor, _predict_index + 1, _tokenizer, _test_model, _seq_len, _title_len, _test_device)\n",
    "\n",
    "def test_model(_text_input: string, \n",
    "               _tokenizer: transformers.PreTrainedTokenizer, \n",
    "               _test_model: transformers.PreTrainedModel, \n",
    "               _seq_len: int, \n",
    "               _title_len: int, \n",
    "               _test_device: torch.device):\n",
    "    with torch.no_grad():\n",
    "        _tok_text = torch.tensor(_tokenizer.encode(_tokenizer.bos_token + _text_input + _tokenizer.sep_token, add_special_tokens=False), dtype=torch.long)\n",
    "        _tok_title = torch.full((title_len,), _tokenizer.mask_token_id, dtype=torch.long)\n",
    "        _predicted_tensor = predict_forward(_text_input, _tok_title, 0, _tokenizer, _test_model, _seq_len, _title_len, _test_device)\n",
    "        return _tokenizer.decode(_predicted_tensor)\n",
    "    \n",
    "print(test_model(article_text, model, CHUNK_SEQ_LEN, TITLE_PRED_MAX_LEN, device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
