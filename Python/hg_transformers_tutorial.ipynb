{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/neonbjb/nonint-transformers\" target=\"_blank\">https://app.wandb.ai/neonbjb/nonint-transformers</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/neonbjb/nonint-transformers/runs/k2po2rq9\" target=\"_blank\">https://app.wandb.ai/neonbjb/nonint-transformers/runs/k2po2rq9</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow_datasets\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "import wandb\n",
    "from wandb.tensorflow import WandbHook\n",
    "wandb.init(project=\"nonint-transformers\", sync_tensorboard=True)\n",
    "\n",
    "from transformers import (TFBertModel, \n",
    "                          BertTokenizer,\n",
    "                          TFRobertaForSequenceClassification, \n",
    "                          RobertaTokenizer)\n",
    "\n",
    "from transformers import glue_convert_examples_to_features\n",
    "\n",
    "fp16 = True\n",
    "BATCH_SIZE = 32\n",
    "if fp16:\n",
    "    tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "    BATCH_SIZE = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "bert_model_pre = TFBertModel.from_pretrained(\"bert-base-cased\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "roberta_model_pre = TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta_model = roberta_model_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT: ['S', '##ys', '##to', '##lic', 'array', '##s', 'are', 'cool', '.', 'This', '[UNK]', 'is', 'cool', 'too', '.']\n",
      "RoBERTa: ['Sy', 'st', 'olic', 'ƒ†arrays', 'ƒ†are', 'ƒ†cool', '.', 'ƒ†This', 'ƒ†√∞≈Å', 'ƒ≤', '¬≥', 'ƒ†is', 'ƒ†cool', 'ƒ†too', '.']\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Systolic arrays are cool. This üê≥ is cool too.\"\n",
    "\n",
    "bert_tokenized_sequence = bert_tokenizer.tokenize(sequence)\n",
    "roberta_tokenized_sequence = roberta_tokenizer.tokenize(sequence)\n",
    "\n",
    "print(\"BERT:\", bert_tokenized_sequence)\n",
    "print(\"RoBERTa:\", roberta_tokenized_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset glue (C:\\Users\\jbetk\\tensorflow_datasets\\glue\\mrpc\\0.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from C:\\Users\\jbetk\\tensorflow_datasets\\glue\\mrpc\\0.0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " idx:       tf.Tensor(201, shape=(), dtype=int32) \n",
      " label:     tf.Tensor(1, shape=(), dtype=int64) \n",
      " sentence1: tf.Tensor(b'Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company .', shape=(), dtype=string) \n",
      " sentence2: tf.Tensor(b'Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .', shape=(), dtype=string)\n",
      " idx:       tf.Tensor(2977, shape=(), dtype=int32) \n",
      " label:     tf.Tensor(0, shape=(), dtype=int64) \n",
      " sentence1: tf.Tensor(b\"Most of the alleged spammers engaged in fraudulent or deceptive practices , said Brad Smith , Microsoft 's senior VP and general counsel .\", shape=(), dtype=string) \n",
      " sentence2: tf.Tensor(b'\" Spam knows no borders , \" said Brad Smith , Microsoft \\'s senior vice-president and general counsel .', shape=(), dtype=string)\n",
      " idx:       tf.Tensor(3482, shape=(), dtype=int32) \n",
      " label:     tf.Tensor(1, shape=(), dtype=int64) \n",
      " sentence1: tf.Tensor(b'Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 .', shape=(), dtype=string) \n",
      " sentence2: tf.Tensor(b'The island reported another 35 probable cases yesterday , taking its total to 418 .', shape=(), dtype=string)\n",
      " idx:       tf.Tensor(1187, shape=(), dtype=int32) \n",
      " label:     tf.Tensor(1, shape=(), dtype=int64) \n",
      " sentence1: tf.Tensor(b'A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter .', shape=(), dtype=string) \n",
      " sentence2: tf.Tensor(b'A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .', shape=(), dtype=string)\n",
      " idx:       tf.Tensor(1220, shape=(), dtype=int32) \n",
      " label:     tf.Tensor(0, shape=(), dtype=int64) \n",
      " sentence1: tf.Tensor(b'Gillespie sent a letter to CBS President Leslie Moonves asking for a historical review or a disclaimer .', shape=(), dtype=string) \n",
      " sentence2: tf.Tensor(b'Republican National Committee Chairman Ed Gillespie issued a letter Friday to CBS Television President Leslie Moonves .', shape=(), dtype=string)\n",
      "BERT tokenizer separator, cls token id:    102 101\n",
      "RoBERTa tokenizer separator, cls token id: 2 0\n",
      "\n",
      "BERT tokenized sequence\n",
      "\u001b[91m101\u001b[0m 23166 1850 170 2998 1106 5957 1697 8521 5148 5710 4107 1111 170 3009 3189 1137 170 6187 20737 4027 119 \u001b[91m102\u001b[0m 3215 1305 2341 4284 5316 23166 3010 170 2998 5286 1106 5957 4552 1697 8521 5148 5710 119 \u001b[91m102\u001b[0m \n",
      "\n",
      "RoBERTa tokenized sequence\n",
      "\u001b[91m0\u001b[0m 534 1873 47712 1051 10 1601 7 3710 270 12504 5648 3677 1996 13 10 4566 1551 50 10 40140 479 \u001b[91m2\u001b[0m \u001b[91m2\u001b[0m 33364 496 1674 3356 2344 25418 1167 10 1601 273 7 3710 10276 270 12504 5648 3677 479 \u001b[91m2\u001b[0m "
     ]
    }
   ],
   "source": [
    "# Fine tune the models\n",
    "import tensorflow_datasets\n",
    "data = tensorflow_datasets.load(\"glue/mrpc\")\n",
    "\n",
    "train_dataset = data[\"train\"]\n",
    "validation_dataset = data[\"validation\"]\n",
    "for i in range(5):\n",
    "    example = list(train_dataset.__iter__())[i]\n",
    "    print('',\n",
    "        'idx:      ', example['idx'],       '\\n',\n",
    "        'label:    ', example['label'],     '\\n',\n",
    "        'sentence1:', example['sentence1'], '\\n',\n",
    "        'sentence2:', example['sentence2'],\n",
    "    )\n",
    "\n",
    "# Decode from a tensor into a UTF-8 string\n",
    "seq0 = example['sentence1'].numpy().decode('utf-8')  # Obtain bytes from tensor and convert it to a string\n",
    "seq1 = example['sentence2'].numpy().decode('utf-8')  # Obtain bytes from tensor and convert it to a string\n",
    "\n",
    "# Encode string into a list of tokens\n",
    "encoded_bert_sequence = bert_tokenizer.encode(seq0, seq1, add_special_tokens=True, max_length=128)\n",
    "encoded_roberta_sequence = roberta_tokenizer.encode(seq0, seq1, add_special_tokens=True, max_length=128)\n",
    "\n",
    "print(\"BERT tokenizer separator, cls token id:   \", bert_tokenizer.sep_token_id, bert_tokenizer.cls_token_id)\n",
    "print(\"RoBERTa tokenizer separator, cls token id:\", roberta_tokenizer.sep_token_id, roberta_tokenizer.cls_token_id)\n",
    "\n",
    "bert_special_tokens = [bert_tokenizer.sep_token_id, bert_tokenizer.cls_token_id]\n",
    "roberta_special_tokens = [roberta_tokenizer.sep_token_id, roberta_tokenizer.cls_token_id]\n",
    "\n",
    "def print_in_red(string):\n",
    "    print(\"\\033[91m\" + str(string) + \"\\033[0m\", end=' ')\n",
    "\n",
    "print(\"\\nBERT tokenized sequence\")\n",
    "output = [print_in_red(tok) if tok in bert_special_tokens else print(tok, end=' ') for tok in encoded_bert_sequence]\n",
    "\n",
    "print(\"\\n\\nRoBERTa tokenized sequence\")\n",
    "output = [print_in_red(tok) if tok in roberta_special_tokens else print(tok, end=' ') for tok in encoded_roberta_sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Go ahead and perform the above steps to all of the train/val dataset.\n",
    "bert_train_dataset = glue_convert_examples_to_features(train_dataset, bert_tokenizer, 128, 'mrpc')\n",
    "bert_train_dataset = bert_train_dataset.shuffle(100).batch(BATCH_SIZE).repeat(2)\n",
    "bert_validation_dataset = glue_convert_examples_to_features(validation_dataset, bert_tokenizer, 128, 'mrpc')\n",
    "bert_validation_dataset = bert_validation_dataset.batch(64)\n",
    "\n",
    "# RoBERTa requires a bit more of work as it does not use the token_type_ids, \n",
    "# which we need to remove. We use the tf.data.Dataset.map() method for this.\n",
    "def token_type_ids_removal(example, label):\n",
    "    del example[\"token_type_ids\"]\n",
    "    return example, label\n",
    "roberta_train_dataset = glue_convert_examples_to_features(train_dataset, roberta_tokenizer, 128, 'mrpc')\n",
    "roberta_train_dataset = roberta_train_dataset.map(token_type_ids_removal)\n",
    "roberta_train_dataset = roberta_train_dataset.shuffle(100).batch(BATCH_SIZE).repeat(2)\n",
    "roberta_validation_dataset = glue_convert_examples_to_features(validation_dataset, roberta_tokenizer, 128, 'mrpc')\n",
    "roberta_validation_dataset = roberta_validation_dataset.map(token_type_ids_removal)\n",
    "roberta_validation_dataset = roberta_validation_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)#, clipnorm=1.0)\n",
    "if fp16:\n",
    "    #optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, \"dynamic\")\n",
    "    tf.train.experimental.enable_mixed_precision_graph_rewrite(optimizer)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "inputs = [Input(shape=(128,), dtype='int32', name='input_ids'),\n",
    "          Input(shape=(128,), dtype='int32', name='attention_mask'), \n",
    "          Input(shape=(128,), dtype='int32', name='token_type_ids')]\n",
    "tensor = bert_model_pre(inputs)[1]\n",
    "tensor = Dense(activation='softmax', units=2)(tensor)\n",
    "bert_model = tf.keras.Model(inputs=inputs, outputs=tensor)\n",
    "\n",
    "bert_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "roberta_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning BERT on MRPC\n",
      "Epoch 1/5\n",
      "154/154 [==============================] - 69s 449ms/step - loss: 0.5294 - accuracy: 0.7762 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.3970 - accuracy: 0.9132WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 154 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 154 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "154/154 [==============================] - 54s 348ms/step - loss: 0.3965 - accuracy: 0.9134 - val_loss: 0.0216 - val_accuracy: 0.8309\n"
     ]
    }
   ],
   "source": [
    "print(\"Fine-tuning BERT on MRPC\")\n",
    "bert_history = bert_model.fit(bert_train_dataset, epochs=5, validation_data=bert_validation_dataset)\n",
    "\n",
    "#print(\"\\nFine-tuning RoBERTa on MRPC\")\n",
    "#roberta_history = roberta_model.fit(roberta_train_dataset, epochs=3, validation_data=roberta_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "wandb.log must be passed a dictionary",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-328949c45b0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwandb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\drive\\projects\\ml-notebooks\\pycharm-venv\\lib\\site-packages\\wandb\\__init__.py\u001b[0m in \u001b[0;36mlog\u001b[1;34m(row, commit, step, sync, *args, **kwargs)\u001b[0m\n\u001b[0;32m    734\u001b[0m             \"You must call `wandb.init` in the same process before calling log\")\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msync\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\drive\\projects\\ml-notebooks\\pycharm-venv\\lib\\site-packages\\wandb\\wandb_run.py\u001b[0m in \u001b[0;36mlog\u001b[1;34m(self, row, commit, step, sync, *args, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"wandb.log must be passed a dictionary\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: wandb.log must be passed a dictionary"
     ]
    }
   ],
   "source": [
    "wandb.log(bert_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Test it out.\n",
    "\n",
    "test_sentence_1 = 'A whale jumped from the ocean, breathing heavily as it floated in the air.'\n",
    "test_sentence_2 = 'Breathing with exertion, the whale flew out of the water into the air.'\n",
    "\n",
    "def pad_zero(inputs, seq_len):\n",
    "    for k in inputs: \n",
    "        output = np.zeros(seq_len+1, dtype='int32')\n",
    "        output[:len(inputs[k])] = np.asarray(inputs[k])\n",
    "        inputs[k] = output\n",
    "    return inputs\n",
    " \n",
    "test_sentence_bert_encoded = pad_zero(bert_tokenizer.encode_plus(test_sentence_1, test_sentence_2, add_special_tokens=True, max_length=128), 128)\n",
    "print(test_sentence_bert_encoded)\n",
    "test_sentence_roberta_encoded = pad_zero(roberta_tokenizer.encode_plus(test_sentence_1, test_sentence_2, add_special_tokens=True, max_length=128), 128)\n",
    "\n",
    "\n",
    "test_sentence_bert_encoded_formatted = \\\n",
    "    [np.resize(test_sentence_bert_encoded['input_ids'], (1,-1)),\n",
    "    np.resize(test_sentence_bert_encoded['token_type_ids'], (1,-1)),\n",
    "    np.resize(test_sentence_bert_encoded['attention_mask'], (1,-1))]\n",
    "print(bert_model.predict(test_sentence_bert_encoded_formatted))\n",
    "\n",
    "#rbs_frm = [np.resize(np.asarray(ers['input_ids']), (1,-1)),\n",
    "#          np.resize(np.asarray(ers['attention_mask']), (1,-1))]\n",
    "#print(roberta_model.predict(rbs_frm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
