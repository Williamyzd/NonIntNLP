{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "125447696"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 1
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "saved_model_dir = \"c:/Users/jbetk/Documents/data/ml/saved_models/sentiment_mse_gp2_yelp_amazon/\"\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "converter.experimental_new_converter = True\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "open(saved_model_dir + \"converted_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Inputs:  [{'name': 'input_ids', 'index': 4418, 'shape': array([  1, 128]), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0)}]\n",
      "Outputs:  [{'name': 'Identity', 'index': 1, 'shape': array([1, 1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "import numpy as np\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=saved_model_dir + \"/converted_model.tflite\")\n",
    "print(\"Inputs: \", interpreter.get_input_details())\n",
    "print(\"Outputs: \", interpreter.get_output_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "inference [1.122301]:  This product ruined my day. Not figuratively, I stuck my fingers together and couldn't get them apart! I would never recommend this to anyone! Luckily I could return it..\n",
      "inference [4.845319]:  This product has been the standard for woodworking pros for decades. Before the 800lb gorilla swaggered into the room, taking the market by storm and strong-arming his way to happily humming registers all over the country, Titebond was THE glue to beat.\n",
      "inference [2.750458]:  good glue, but had hardened glue stuck in the spout and is clogged have to unscrew and use a popsicle stick to use, i will try and use isopropyl to dissolve it so i can use the spout, kinda annoying\n",
      "inference [3.275617]:  Strong as can be, but stain will discolor. Make sure you wipe it off completely before it dries.\n",
      "inference [1.343117]:  Did not use this glue immediately. About 4 weeks later, I could not get any glue to squeeze out thru the dispenser tip. I removed the cap and discovered lumps in the glue.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "phrases = [\n",
    "    \" This product ruined my day. Not figuratively, I stuck my fingers together and couldn't get them apart! I would never recommend this to anyone! Luckily I could return it..\",\n",
    "    # 5 star:\n",
    "    \" This product has been the standard for woodworking pros for decades. Before the 800lb gorilla swaggered into the room, taking the market by storm and strong-arming his way to happily humming registers all over the country, Titebond was THE glue to beat.\",\n",
    "    # 4 star:\n",
    "    \" good glue, but had hardened glue stuck in the spout and is clogged have to unscrew and use a popsicle stick to use, i will try and use isopropyl to dissolve it so i can use the spout, kinda annoying\",\n",
    "    # 3 star:\n",
    "    \" Strong as can be, but stain will discolor. Make sure you wipe it off completely before it dries.\",\n",
    "    # 2 star:\n",
    "    \" Did not use this glue immediately. About 4 weeks later, I could not get any glue to squeeze out thru the dispenser tip. I removed the cap and discovered lumps in the glue.\",\n",
    "]\n",
    "\n",
    "def pad_zero(inputs, seq_len):\n",
    "    for k in inputs: \n",
    "        output = np.zeros(seq_len, dtype='int32')\n",
    "        output[:len(inputs[k])] = np.asarray(inputs[k])\n",
    "        inputs[k] = output\n",
    "    return inputs\n",
    "\n",
    "for phrase in phrases:\n",
    "    enc = pad_zero(tokenizer.encode_plus(phrase, add_special_tokens=True, max_length=128), 128)\n",
    "    phrase_encoded = []\n",
    "    for k in enc.keys():\n",
    "        phrase_encoded.append(enc[k])\n",
    "        \n",
    "    interpreter.reset_all_variables()\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    np.copyto(interpreter.tensor(interpreter.get_input_details()[0][\"index\"])(), phrase_encoded[0])\n",
    "    interpreter.invoke()\n",
    "    print((\"inference [%f]: \" + phrase) % (interpreter.tensor(interpreter.get_output_details()[0][\"index\"])()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}