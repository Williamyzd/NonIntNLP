{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "from transformers import (TFBertModel, \n",
    "                          BertTokenizer,\n",
    "                          glue_convert_examples_to_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 128\n",
    "EPOCHS = 3\n",
    "\n",
    "# FP16 settings\n",
    "fp16 = True\n",
    "if fp16:\n",
    "    tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "    BATCH_SIZE = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fetch pre-trained models\n",
    "\n",
    "bert_base_model = TFBertModel.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fetch and format dataset.\n",
    "def downconvert_tf_dataset(dataset, tok, pad_token=0):\n",
    "    inputs = []\n",
    "    atts = []\n",
    "    toks = []\n",
    "    outputs = []\n",
    "    for i,m in enumerate(dataset):\n",
    "        input = tok.encode_plus(m['sentence'].numpy().decode(\"utf-8\"),\\\n",
    "                                      add_special_tokens=True, max_length=MAX_SEQ_LEN,)\n",
    "        input_ids, token_type_ids = input[\"input_ids\"], input[\"token_type_ids\"]\n",
    "        attention_mask = [0] * len(input_ids)\n",
    "        \n",
    "        # Pad strings to exactly MAX_SEQ_LEN\n",
    "        padding_length = MAX_SEQ_LEN - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token] * padding_length)\n",
    "        attention_mask = attention_mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "\n",
    "        # Double-check results.\n",
    "        assert len(input_ids) == MAX_SEQ_LEN, \"Error with input length {} vs {}\".format(len(input_ids), max_length)\n",
    "        assert len(attention_mask) == MAX_SEQ_LEN, \"Error with input length {} vs {}\".format(\n",
    "            len(attention_mask), MAX_SEQ_LEN\n",
    "        )\n",
    "        assert len(token_type_ids) == MAX_SEQ_LEN, \"Error with input length {} vs {}\".format(\n",
    "            len(token_type_ids), MAX_SEQ_LEN\n",
    "        )\n",
    "        \n",
    "        # Form lists.\n",
    "        inputs.append(np.asarray(input_ids))\n",
    "        atts.append(np.asarray(attention_mask))\n",
    "        toks.append(np.asarray(token_type_ids))\n",
    "        outputs.append(m['label'].numpy())\n",
    "    return [np.asarray(inputs), np.asarray(atts), np.asarray(toks)], np.asarray(outputs)\n",
    "\n",
    "sst_data = tensorflow_datasets.load(\"glue/sst2\")\n",
    "sst_train_x, sst_train_y = downconvert_tf_dataset(sst_data[\"train\"], tokenizer)\n",
    "sst_val_x, sst_val_y = downconvert_tf_dataset(sst_data[\"validation\"], tokenizer)\n",
    "\n",
    "'''\n",
    "# We can work in-place with a TF dataset, but working with them is a pain and there are bugs:\n",
    "#  Doesn't complete some epochs with \"out of data\" error\n",
    "#  Doesn't track training properly\n",
    "#  Doesn't use validation set properly\n",
    "train_dataset = glue_convert_examples_to_features(data[\"train\"], tokenizer, MAX_SEQ_LEN, 'sst-2')\\\n",
    "    .shuffle(1337)\\\n",
    "    .batch(BATCH_SIZE)\\\n",
    "    .repeat(EPOCHS)\n",
    "validation_dataset = glue_convert_examples_to_features(data[\"validation\"], tokenizer, MAX_SEQ_LEN, 'sst-2')\\\n",
    "    .batch(BATCH_SIZE)\n",
    "    '''\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configure and compile model.\n",
    "\n",
    "# Later cells might set trainable=False; which we don't necessarily want here.\n",
    "bert_base_model.trainable = True\n",
    "sst_inputs = [Input(shape=(128,), dtype='int32', name='input_ids'),\n",
    "          Input(shape=(128,), dtype='int32', name='attention_mask'), \n",
    "          Input(shape=(128,), dtype='int32', name='token_type_ids')]\n",
    "# Fetch the CLS head of the BERT model; index 1.\n",
    "sst_tensor = bert_base_model(sst_inputs)[1]\n",
    "#sst_tensor = Dense(activation='softmax', units=256)(sst_tensor)\n",
    "sst_tensor = Dense(activation='softmax', units=2)(sst_tensor)\n",
    "sst_bert_model = keras.Model(inputs=sst_inputs, outputs=sst_tensor)\n",
    "print(sst_bert_model.summary())\n",
    "\n",
    "# Configure optimizer, loss function and metrics.\n",
    "sst_optimizer_base_model = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "sst_optimizer_head = tf.keras.optimizers.Adam()\n",
    "if fp16:\n",
    "    tf.train.experimental.enable_mixed_precision_graph_rewrite(sst_optimizer_base_model)\n",
    "    tf.train.experimental.enable_mixed_precision_graph_rewrite(sst_optimizer_head)\n",
    "sst_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "sst_metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "#sst_bert_model.compile(optimizer=[sst_optimizer_base_model, sst_optimizer_head], loss=sst_loss, metrics=[sst_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell defines a utility function for easily tracking progress in jupyter.\n",
    "\n",
    "import time, sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def update_progress(epoch, progress, msg):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    text = \"Epoch {0}: [{1}] {2:.1f}% {3}\".format(epoch, \"#\" * block + \"-\" * (bar_length - block), \\\n",
    "                                                  progress * 100, msg)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "\n",
    "def train_step(model, loss, optimizers, optimizer_vars, x_vals, y_vals):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        logits = model(x_vals, training=True)\n",
    "        loss_value = loss(y_vals, logits)\n",
    "    loss_scalar = loss_value.numpy().mean()\n",
    "    for (optimizer, vrz) in zip(optimizers, optimizer_vars):\n",
    "        grads = tape.gradient(loss_value, vrz)\n",
    "        optimizer.apply_gradients(zip(grads, vrz))\n",
    "    del tape\n",
    "    return loss_scalar\n",
    "\n",
    "def train(model, loss, optimizers, optimizer_vars, x_vals, y_vals, epochs, batch_sz):\n",
    "    # Garbage collect before starting training to attempt to free up GPU memory.\n",
    "    gc.collect()\n",
    "    loss_history = []\n",
    "    for epoch in range(epochs):\n",
    "        training_data_count = y_vals.shape[0]\n",
    "        for batch_num in range(int(training_data_count / batch_sz)):\n",
    "            ii = batch_num * batch_sz\n",
    "            li = ii + batch_sz\n",
    "            li = training_data_count-1 if (li >= training_data_count) else li\n",
    "            # ii=initial index, li=last index. now create batches. remember that x_vals is a list of inputs.\n",
    "            batch_x = [x_vals_ele[ii:li] for x_vals_ele in x_vals]\n",
    "            batch_y = y_vals[ii:li]\n",
    "            loss_history.append(train_step(model, loss, optimizers, optimizer_vars, batch_x, batch_y))\n",
    "            loss_mean = loss_history[-1] if (len(loss_history) < 10) else mean(loss_history[-10:-1])\n",
    "            update_progress(epoch, ii/training_data_count, \\\n",
    "                            \"Loss=%f\" % (loss_history[-1]))\n",
    "    return loss_history\n",
    "            \n",
    "def find_tf_variables_not_in_list(full_variable_list, diff_list):\n",
    "    # Since tensors can't be compared directly, extract their names and use those as keys instead.\n",
    "    fvl_names = [v.name for v in full_variable_list]\n",
    "    dl_names = [v.name for v in diff_list]\n",
    "    diff_names = [n for n in fvl_names if (n not in dl_names)]\n",
    "    return [v for v in full_variable_list if (v.name in diff_names)]\n",
    "\n",
    "head_variables = find_tf_variables_not_in_list(sst_bert_model.trainable_variables, bert_base_model.trainable_variables)\n",
    "sst_bert_history = train(sst_bert_model, sst_loss, [sst_optimizer_base_model, sst_optimizer_head],\\\n",
    "                        [bert_base_model.trainable_variables, head_variables], sst_train_x, sst_train_y,\\\n",
    "                        EPOCHS, 16)\n",
    "# Train model.\n",
    "#sst_bert_history = sst_bert_model.fit(sst_train_x, sst_train_y, epochs=EPOCHS, \\\n",
    "#                                      validation_data=(sst_val_x, sst_val_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sst_bert_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition dataset to another task; freeze BERT model; re-train new head.\n",
    "\n",
    "data_cola = tensorflow_datasets.load(\"glue/cola\")\n",
    "#print(list(data_cola[\"validation\"].__iter__())[0:5])\n",
    "cola_train_x, cola_train_y = downconvert_tf_dataset(data_cola[\"train\"], tokenizer)\n",
    "cola_val_x, cola_val_y = downconvert_tf_dataset(data_cola[\"validation\"], tokenizer)\n",
    "\n",
    "cola_optimizer = VariantRateAdam(name=\"Adam\")\n",
    "if fp16:\n",
    "    tf.train.experimental.enable_mixed_precision_graph_rewrite(cola_optimizer)\n",
    "cola_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "cola_metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "# Lock down the bert model. The intent is that the previous model trained this one.\n",
    "bert_base_model.trainable = False\n",
    "cola_inputs = [Input(shape=(128,), dtype='int32', name='input_ids'),\n",
    "          Input(shape=(128,), dtype='int32', name='attention_mask'), \n",
    "          Input(shape=(128,), dtype='int32', name='token_type_ids')]\n",
    "# Fetch the CLS head of the BERT model; index 1.\n",
    "cola_tensor = bert_base_model(cola_inputs)[1]\n",
    "cola_tensor = Dense(activation='softmax', units=256)(cola_tensor)\n",
    "cola_tensor = Dense(activation='softmax', units=2)(cola_tensor)\n",
    "cola_bert_model = keras.Model(inputs=cola_inputs, outputs=cola_tensor)\n",
    "print(cola_bert_model.summary())\n",
    "\n",
    "cola_bert_model.compile(optimizer=cola_optimizer, loss=cola_loss, metrics=[cola_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cola_bert_history = cola_bert_model.fit(cola_train_x, cola_train_y, epochs=EPOCHS, \\\n",
    "                                      validation_data=(cola_val_x, cola_val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
