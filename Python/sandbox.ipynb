{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from utils import downconvert_tf_dataset\n",
    "\n",
    "from transformers import (TFBertModel, \n",
    "                          BertTokenizer,\n",
    "                          glue_convert_examples_to_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "{('m', 'e'), ('t', 'h'), ('i', 'n'), ('o', 'm'), ('s', 'o'), ('n', 'g'), ('e', 't'), ('h', 'i')}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "print(get_pairs(\"something\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 128\n",
    "EPOCHS = 3\n",
    "\n",
    "# FP16 settings\n",
    "fp16 = True\n",
    "if fp16:\n",
    "    tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "    BATCH_SIZE = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fetch pre-trained models\n",
    "bert_base_model = TFBertModel.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_classification_head(dataset_name, base_model_cls_head, dense_config=[256,2]):\n",
    "    # Fetch the data.\n",
    "    data = tensorflow_datasets.load(dataset_name)\n",
    "    train_x, train_y = downconvert_tf_dataset(data[\"train\"], tokenizer, MAX_SEQ_LEN)\n",
    "    val_x, val_y = downconvert_tf_dataset(data[\"validation\"], tokenizer, MAX_SEQ_LEN)\n",
    "    print(\"Dataset %s train_sz=%i val_sz=%i\" % \\\n",
    "          (dataset_name, train_y.shape[0], val_y.shape[0]))\n",
    "    \n",
    "    # Create the head.\n",
    "    tensor = base_model_cls_head\n",
    "    for layer_units in dense_config[0:-1]:\n",
    "        tensor = Dense(units=layer_units, activation=\"relu\", name=\"%s_%i\" % (dataset_name, layer_units))(tensor)\n",
    "    tensor = Dense(units=dense_config[-1], activation=\"softmax\", name=\"final_%s\" % (dataset_name))(tensor)\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y, tensor\n",
    "\n",
    "def join_np_arrays_with_starting_none(l_a, l_b):\n",
    "    # l_a can be None.\n",
    "    if l_a is None:\n",
    "        return l_b\n",
    "    else:\n",
    "        return np.concatenate((l_a, l_b), axis=0)\n",
    "\n",
    "def create_joint_classification_heads(datasets, base_model_cls_head, cls_task_selector_input,\\\n",
    "                                      dense_config=[256,2]):\n",
    "    cls_heads = []\n",
    "    train_x, train_y, val_x, val_y = None, None, None, None\n",
    "    for (i, dataset) in enumerate(datasets):\n",
    "        # tx and vx are lists of inputs which include the unique transformer inputs for the underlying\n",
    "        #  model.\n",
    "        tx, ty, vx, vy, h = create_new_classification_head(dataset, base_model_cls_head, dense_config)\n",
    "        \n",
    "        # need to append in a signal to tx and vx which indicates which classification task the data\n",
    "        #  comes from. This will be a one-hot array which the multi-head output will be multiplied against.\n",
    "        cls_task_id = np.zeros(len(datasets))\n",
    "        cls_task_id[i] = 1\n",
    "        tx.append(np.broadcast_to(cls_task_id, (ty.shape[0], len(datasets))))\n",
    "        vx.append(np.broadcast_to(cls_task_id, (vy.shape[0], len(datasets))))\n",
    "        \n",
    "        # next, append the datasets and new head into the final produced list.\n",
    "        if train_x is None:\n",
    "            train_x = [None for i in range(len(tx))]\n",
    "            val_x = [None for i in range(len(tx))]\n",
    "        for i, _ in enumerate(train_x):\n",
    "            train_x[i] = join_np_arrays_with_starting_none(train_x[i], tx[i])\n",
    "            val_x[i] = join_np_arrays_with_starting_none(val_x[i], vx[i])\n",
    "        train_y = join_np_arrays_with_starting_none(train_y, ty)\n",
    "        val_y = join_np_arrays_with_starting_none(val_y, vy)\n",
    "        cls_heads.append(h)\n",
    "    \n",
    "    # shuffle the joined training datasets\n",
    "    randomize = np.arange(len(train_y))\n",
    "    np.random.shuffle(randomize)\n",
    "    for i in range(len(train_x)):\n",
    "        train_x[i] = train_x[i][randomize]\n",
    "    train_y = train_y[randomize]\n",
    "    \n",
    "    # Reshape classification heads to all have a concatenation dimension\n",
    "    cls_heads_shaped = []\n",
    "    for head in cls_heads:\n",
    "        # TODO: TEST changing \"2\" (hardcoded) to \"-1\"\n",
    "        cls_heads_shaped.append(keras.layers.Reshape((1,2))(head))\n",
    "    # join the classification heads into a single output.\n",
    "    cls_heads_cat = None\n",
    "    if len(cls_heads) > 1:\n",
    "        cls_heads_cat = keras.layers.Concatenate(axis=1)(cls_heads_shaped)\n",
    "    else:\n",
    "        cls_heads_cat = cls_heads_shaped[0]\n",
    "    print(\"cls_heads_cat:\",K.int_shape(cls_heads_cat))\n",
    "    print(\"cls_selector:\",K.int_shape(cls_task_selector_input))\n",
    "    cls_output = keras.layers.Dot(name=\"cls_head_join\", axes=(1,1))([cls_task_selector_input, cls_heads_cat])\n",
    "    \n",
    "    return train_x,train_y,val_x,val_y,cls_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset glue (C:\\Users\\jbetk\\tensorflow_datasets\\glue\\sst2\\0.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from C:\\Users\\jbetk\\tensorflow_datasets\\glue\\sst2\\0.0.2\n",
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset glue (C:\\Users\\jbetk\\tensorflow_datasets\\glue\\cola\\0.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from C:\\Users\\jbetk\\tensorflow_datasets\\glue\\cola\\0.0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue/sst2 train_sz=67349 val_sz=872\n",
      "Dataset glue/cola train_sz=8551 val_sz=1043\n",
      "cls_heads_cat: (None, 2, 2)\n",
      "cls_selector: (None, 2)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 128, 768), ( 108310272   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "glue/sst2_256 (Dense)           (None, 256)          196864      tf_bert_model[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "glue/cola_256 (Dense)           (None, 256)          196864      tf_bert_model[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "final_glue/sst2 (Dense)         (None, 2)            514         glue/sst2_256[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final_glue/cola (Dense)         (None, 2)            514         glue/cola_256[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 2)         0           final_glue/sst2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 2)         0           final_glue/cola[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "classification_task_selector (I [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2, 2)         0           reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cls_head_join (Dot)             (None, 2)            0           classification_task_selector[0][0\n",
      "                                                                 concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 108,705,028\n",
      "Trainable params: 108,705,028\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Configure and compile model.\n",
    "\n",
    "dataset_ids=[\"glue/sst2\", \"glue/cola\"]\n",
    "\n",
    "# Later cells might set trainable=False; which we don't necessarily want here.\n",
    "inputs = [Input(shape=(128,), dtype='int32', name='input_ids'),\n",
    "          Input(shape=(128,), dtype='int32', name='attention_mask'), \n",
    "          Input(shape=(128,), dtype='int32', name='token_type_ids'),\n",
    "          Input(shape=(len(dataset_ids),), dtype='float32', name='classification_task_selector')]\n",
    "\n",
    "# Fetch the CLS head of the BERT model; index 1.\n",
    "cls_head = bert_base_model(inputs[0:2])[1]\n",
    "\n",
    "# Fetch and format dataset and classification head.\n",
    "sst_train_x, sst_train_y, sst_val_x, sst_val_y, sst_tensor = \\\n",
    "    create_joint_classification_heads(dataset_ids, cls_head, inputs[3], dense_config=[256,2])\n",
    "\n",
    "sst_bert_model = keras.Model(inputs=inputs, outputs=sst_tensor)\n",
    "print(sst_bert_model.summary())\n",
    "\n",
    "# Configure optimizer, loss function and metrics.\n",
    "sst_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "if fp16:\n",
    "    tf.train.experimental.enable_mixed_precision_graph_rewrite(sst_optimizer)\n",
    "sst_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "sst_metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "sst_bert_model.compile(optimizer=sst_optimizer, loss=sst_loss, metrics=[sst_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75900 samples, validate on 1915 samples\n",
      "Epoch 1/3\n",
      "75900/75900 [==============================] - 567s 7ms/sample - loss: 0.4926 - accuracy: 0.8084 - val_loss: 0.5313 - val_accuracy: 0.7791\n",
      "Epoch 2/3\n",
      "75900/75900 [==============================] - 552s 7ms/sample - loss: 0.4234 - accuracy: 0.8872 - val_loss: 0.5291 - val_accuracy: 0.7796\n",
      "Epoch 3/3\n",
      "75900/75900 [==============================] - 552s 7ms/sample - loss: 0.4152 - accuracy: 0.8955 - val_loss: 0.5414 - val_accuracy: 0.7671\n"
     ]
    }
   ],
   "source": [
    "# Train model.\n",
    "sst_bert_history = sst_bert_model.fit(sst_train_x, sst_train_y, batch_size=BATCH_SIZE, epochs=EPOCHS, \\\n",
    "                                      validation_data=(sst_val_x, sst_val_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000e+00 7.8082085e-06]]\n"
     ]
    }
   ],
   "source": [
    "#phrase = \"I was disappointed to see the credits roll, the film really had me.\"\n",
    "phrase = \"A human there was she walked it\"\n",
    "\n",
    "def pad_zero(inputs, seq_len):\n",
    "    for k in inputs: \n",
    "        output = np.zeros(seq_len+1, dtype='int32')\n",
    "        output[:len(inputs[k])] = np.asarray(inputs[k])\n",
    "        inputs[k] = output\n",
    "    return inputs\n",
    " \n",
    "phrase_encoded = pad_zero(tokenizer.encode_plus(phrase, add_special_tokens=True, max_length=128), 128)\n",
    "\n",
    "phrase_encoded_formatted = \\\n",
    "    [np.resize(phrase_encoded['input_ids'], (1,-1)),\n",
    "    np.resize(phrase_encoded['token_type_ids'], (1,-1)),\n",
    "    np.resize(phrase_encoded['attention_mask'], (1,-1)),\n",
    "    np.asarray([[1,0]], dtype='float32')]\n",
    "print(sst_bert_model.predict(phrase_encoded_formatted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}